---
title: "04_Sens_Keep_extremeValues"
---

# Set Parameters and Load Packages
```{r, warning=FALSE}
library(readr)
library(haven)
library(dplyr)
library(ggplot2)
library(DataExplorer)
library(patchwork)
library(here)
source('DataInput_Distribution_Association.R')

custom_theme <- theme(
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  axis.text = element_text(size = 10),
  axis.title = element_text(size = 12, face = "bold"),
  axis.line = element_line(size = 0.5),
  plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
)

custom_theme2 <- theme(
    panel.grid.major = element_blank(),  
    panel.grid.minor = element_blank(),  
    panel.background = element_blank(),  
    axis.ticks = element_blank(),  
    panel.border = element_rect(colour = "black", fill = NA, size = 1)
  )

step_algorithms = c('ADEPT', 'Oak', 'SDT', 'Stepcount', 'Verisense')
```

# 01.Exclude extreme values by 1% and 99%.
## Load data
```{r, warning=FALSE}
dat_vis = read_csv(here('00_Intermediate_output/03_Step_Cov_Imputed.csv'))
dat_vis$...1 = NULL
```
## Exclude extreme values
```{r, warning=FALSE}
result = dat_vis
ex.list = c()
for (var in step_algorithms){
  exetreme.lower = quantile(result[[var]], c(0.01, 0.99))[1]
  exetreme.upper = quantile(result[[var]], c(0.01, 0.99))[2]
  list.temp = result[result[[var]] > exetreme.upper | result[[var]] < exetreme.lower,]$subjectid
  ex.list = c(ex.list, list.temp)
}
result.noextreme = result[result$subjectid %in% setdiff(result$subjectid, unique(ex.list)),] # N=62 excluded
cat('The number of individuals who are excluded due to extreme values: ', 
    dim(result)[1] - dim(result.noextreme)[1])
```

## Distribution results
```{r, warning=FALSE}
dat.step = result.noextreme[,c(1,3:7)]/1000
step.distribution.subtitle(dat.step, 'Analysis_Sensitivity/Sens.Exclude_extreme_By10')
step.distribution(dat.step, 'Analysis_Sensitivity/Sens.Exclude_extreme_By10')
```
# 02.Exclude missing smoking data
## Load data
```{r, warning=FALSE}
dat_vis = read_csv(here('00_Intermediate_output/09_Step_Cov_WithoutSmokingImpute.csv'))
dat_vis$...1 = NULL
dat_vis = dat_vis |> filter(!is.na(Smoking))
```

## Exclude extreme values
by Adaptive Trimmed Mean Estimator
```{r, warning=FALSE}
result = dat_vis
ex.list = c()
for (var in step_algorithms){
  adjbox_result <- adjbox(result[[var]], id.n = Inf, plot = TRUE)
  list.temp = which(result[[var]]  %in%  adjbox_result$out)
  ex.list = c(ex.list, list.temp)
}
ex.list = result$subjectid[ex.list]

result.noextreme = result[result$subjectid %in% setdiff(result$subjectid, # N=1215
                                                        unique(ex.list)),] # N=37 Excluded
cat('The number of individuals who are excluded due to extreme values: ',
    dim(result)[1] - dim(result.noextreme)[1])
```

## Distribution results
```{r, warning=FALSE}
dat.step = result.noextreme[,c(1,3:7)]
step.distribution.subtitle(dat.step, 'Analysis_Sensitivity/Sens.ExcludeSmoking_Association')
step.distribution(dat.step, 'Analysis_Sensitivity/Sens.ExcludeSmoking_Association')
```

## Association results
```{r, warning=FALSE}
dat_cov = dat_vis
num = which(colnames(dat_cov) %in% step_algorithms)
dat_cov.per = dat_cov; dat_cov.per[, num] = dat_cov.per[, num]/1000
association.result(dat_cov.per, 'Analysis_Sensitivity/Sens.ExcludeSmoking_Association/')

dat_cov.scale = dat_cov; dat_cov.scale[, num] = scale(dat_cov.scale[,num])
association.result(dat_cov.scale, 'Analysis_Sensitivity/Sens.ExcludeSmoking_Association/scaled/')
```

# 03.Statified by sex and race
## Load data
```{r}
dat_cov = read_csv(here('00_Intermediate_output/03_Step_Cov_Imputed.csv'))
```

##Calculate the mean and standard deviation for each subgroup
```{r}
results <- dat_cov %>%
  group_by(Sex, Race) %>%
  summarise(across(all_of(step_algorithms), 
                   list(mean = ~mean(., na.rm = TRUE),
                        sd = ~sd(., na.rm = TRUE)))) %>%
  ungroup()

results_long <- results %>%
  pivot_longer(cols = -c(Sex, Race), 
               names_to = c("method", ".value"), 
               names_pattern = "(.+)_(.+)")

formatted_table <- results_long %>%
  mutate(value = sprintf("%.2f Â± %.2f", mean, sd)) %>%
  select(-mean, -sd) %>%
  pivot_wider(names_from = c(Sex, Race), values_from = value)
```

## Analyze demographic differences
```{r}
formatted_table$Anova = NULL
for(method in step_algorithms) {
  cat("\nANOVA for", method, ":\n")
  model <- aov(as.formula(paste(method, "~ Sex * Race")), data = dat_cov)
  print(summary(model)); tmp = summary(model)
  formatted_table[formatted_table$method == method, 'Anova'] = round(tmp[[1]][3,5],3)
}

formatted_table$method = c('ADEPT', 'OAK', 'SDT', 'Stepcount', 'Verisence')
writexl::write_xlsx(formatted_table, 
                    here('results/Analysis_Sensitivity/Sens.Stratified.Scale/steps.stratified.summary.xlsx'))
```

## ANOVA and two-factor analysis
```{r}
library(car)
library(emmeans)

# Analyze demographic differences
for(method in step_algorithms) {
  cat("\n\nAnalysis for", method, ":\n")
  
  # Check homogeneity of variance assumption
  levene_test <- leveneTest(as.formula(paste(method, "~ Sex * Race")), data = dat_cov)
  cat("Levene's Test for Homogeneity of Variance:\n")
  print(levene_test)
  
  # Perform two-way ANOVA
  model <- aov(as.formula(paste(method, "~ Sex * Race")), data = dat_cov)
  cat("\nANOVA Results:\n")
  print(summary(model))
  
  # For significant interaction, conduct Tukey Test
  if(any(summary(model)[[1]]$`Pr(>F)`[3] < 0.1)) {  # check the significance of interaction term
    emm_interaction <- emmeans(model, ~ Sex:Race)
    tukey_interaction <- pairs(emm_interaction)
    cat("\nTukey HSD for Sex:Race Interaction:\n")
    print(tukey_interaction)
  }
  
  cat("\n-----------------------------------\n")
}
```

## Distribution results
```{r, warning=FALSE}
dat.step = dat_cov[,c(1,3:7)]
step.distribution.subtitle(dat.step, 'Analysis_Sensitivity/Sens.Stratified.Scale')
step.distribution(dat.step, 'Analysis_Sensitivity/Sens.Stratified.Scale')
```

## Association results
### Stratified by sex and race, and normalize within each stratum
```{r}
dt.all = read_csv(here('00_Intermediate_output/03_Step_Cov_Imputed.csv'))

# Stratify by sex and race, and normalize within each stratum
for(method in step_algorithms) {
  dt.all <- dt.all %>%
    group_by(Sex, Race) %>%
    mutate(!!paste0(method, "") := scale(!!sym(method))) %>%
    ungroup()
}

association.result(dt.all, 'Analysis_Sensitivity/Sens.Stratified.Scale/')
```

#04.Cross validation (Fold=3)
## Load data
```{r}
dt.all = read_csv(here('00_Intermediate_output/03_Step_Cov_Imputed.csv'))
```

## Table 1
```{r}
set.seed(1)
part.num = 3
n <- nrow(dt.all)
group_size <- n %/% part.num; remainder <- n %% part.num

# generate group vectors
groups <- rep(1:part.num, each = group_size)
if(remainder > 0) {
  groups <- c(groups, sample(1:part.num, remainder))
}
df <- dt.all %>% mutate(group = sample(groups))

## Sample characteristics (Like Table 1)
df$Sex = as.factor(df$Sex)
df$Race = as.factor(df$Race)
df$Center = as.factor(df$Center)
df$Education = as.factor(df$Education)
df$Smoking = as.factor(df$Smoking)
df$Drinking = as.factor(df$Drinking)
df$APOE = as.factor(df$APOE)
df$group = as.factor(df$group)

df %>% 
  select(exam_age, Sex, Race, Center, Education,
         Smoking, Drinking, APOE, ADEPT,
         Oak, SDT, Stepcount, Verisense, group) %>%
  tbl_summary(
    by = "group",
    statistic = all_continuous() ~ "{mean}({sd})",
    type = list(c(ADEPT, Oak, SDT, Stepcount, Verisense) ~ "continuous")
  ) %>%
  add_overall() %>%
  add_p(test = list(all_continuous() ~ "aov",
                    all_categorical() ~ "chisq.test")) %>%
  as_flex_table() %>%
  save_as_docx(path = here("results/Analysis_Sensitivity/Sens.CV5.Associations/Table1.CV.docx"))
```

## Association results
```{r}
num = which(colnames(df) %in% step_algorithms)

for (id in c(1:3)){
  output.folder = paste0('Analysis_Sensitivity/Sens.CV5.Associations/', 
                              'Fold-', id, '/')
  output.folder.scale = paste0('Analysis_Sensitivity/Sens.CV5.Associations/', 
                                    'Fold-', id, '/scaled/')
  
  df.tmp = df |> filter(group == id) |> select(-group)
  dat_cov.per = df.tmp; dat_cov.per[, num] = dat_cov.per[, num]/1000
  association.result(dat_cov.per, output.folder)
  
  dat_cov.scale = df.tmp; dat_cov.scale[, num] = scale(dat_cov.scale[,num])
  association.result(dat_cov.scale, output.folder.scale)
}